{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame -> Tensorflow Dataset\n",
    "\n",
    "This notebook serves as a playground for testing `oarphpy.spark.spark_df_to_tf_dataset()`.  See also the unit tests for this utiltiy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/google/protobuf/__init__.py:37: UserWarning: Module oarphpy was already imported from /opt/oarphpy/oarphpy/__init__.py, but /opt/oarphpy/notebooks is being added to sys.path\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n",
      "2019-12-27 21:01:03,560\toarph 336 : Trying to auto-resolve path to src root ...\n",
      "2019-12-27 21:01:03,561\toarph 336 : Using source root /opt/oarphpy \n",
      "2019-12-27 21:01:03,589\toarph 336 : Generating egg to /tmp/op_spark_eggs_e2392756-5287-4e0e-bdb3-3bc52ee6cde4 ...\n",
      "2019-12-27 21:01:03,641\toarph 336 : ... done.  Egg at /tmp/op_spark_eggs_e2392756-5287-4e0e-bdb3-3bc52ee6cde4/oarphpy-0.0.0-py3.6.egg\n"
     ]
    }
   ],
   "source": [
    "# Common imports and setup\n",
    "from oarphpy.spark import NBSpark\n",
    "from oarphpy.spark import spark_df_to_tf_dataset\n",
    "from oarphpy import util\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = NBSpark.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a \"large\" 2GB random dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RECORDS = 1000\n",
    "\n",
    "DATASET_PATH = '/tmp/spark_df_to_tf_dataset_test_large'\n",
    "def gen_data(n):\n",
    "  import numpy as np\n",
    "  y = np.random.rand(2 ** 15).tolist()\n",
    "  return Row(part=n % 100, id=str(n), x=1, y=y)\n",
    "rdd = spark.sparkContext.parallelize(range(NUM_RECORDS))\n",
    "df = spark.createDataFrame(rdd.map(gen_data))\n",
    "if util.missing_or_empty(DATASET_PATH):\n",
    "    df.write.parquet(DATASET_PATH, partitionBy=['part'], mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7M\t/tmp/spark_df_to_tf_dataset_test_large\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$DATASET_PATH\"\n",
    "du -sh $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test reading the dataset through Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 10 rows\n",
      "getting shards\n",
      "10 [1, 6, 3, 5, 9, 4, 8, 7, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-27 21:02:21,279\toarph 336 : Reading partition 3 \n",
      "2019-12-27 21:02:21,280\toarph 336 : Reading partition 0 \n",
      "2019-12-27 21:02:21,281\toarph 336 : Reading partition 1 \n",
      "2019-12-27 21:02:21,281\toarph 336 : Reading partition 6 \n",
      "2019-12-27 21:02:21,283\toarph 336 : Reading partition 8 \n",
      "2019-12-27 21:02:21,284\toarph 336 : Reading partition 4 \n",
      "2019-12-27 21:02:21,287\toarph 336 : Reading partition 5 \n",
      "2019-12-27 21:02:21,287\toarph 336 : Reading partition 2 \n",
      "2019-12-27 21:02:21,288\toarph 336 : Reading partition 7 \n",
      "2019-12-27 21:02:21,294\toarph 336 : Reading partition 9 \n",
      "2019-12-27 21:02:30,044\toarph 336 : Done reading partition 5, stats:\n",
      " Partition 5 [Pid:336 Id:140641112145760]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.73 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        90.11 KB / sec\n",
      "Hz          0.11456897938921241\n",
      "----------  -------------------\n",
      "2019-12-27 21:02:30,047\toarph 336 : Progress for \n",
      "spark_tf_dataset [Pid:336 Id:140639257511920]\n",
      "-----------------------  --------------------------\n",
      "Thruput\n",
      "N thru                   1 (of 10)\n",
      "N chunks                 1\n",
      "Total time               9.28 seconds\n",
      "Total thru               786.52 KB\n",
      "Rate                     84.72 KB / sec\n",
      "Hz                       0.10771379278642114\n",
      "Progress\n",
      "Percent Complete         10.0\n",
      "Est. Time To Completion  1 minute and 23.55 seconds\n",
      "-----------------------  --------------------------\n",
      "2019-12-27 21:02:30,049\toarph 336 : \n",
      "Partition 5 [Pid:336 Id:140641112145760]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.73 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        90.11 KB / sec\n",
      "Hz          0.11456897938921241\n",
      "----------  -------------------\n",
      "\n",
      "2019-12-27 21:02:30,050\toarph 336 : Done reading partition 4, stats:\n",
      " Partition 4 [Pid:336 Id:140641112145704]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.76 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        89.8 KB / sec\n",
      "Hz          0.11417568922247091\n",
      "----------  -------------------\n",
      "2019-12-27 21:02:30,063\toarph 336 : Progress for \n",
      "spark_tf_dataset [Pid:336 Id:140639257511920]\n",
      "-----------------------  ---------------------------------\n",
      "Thruput\n",
      "N thru                   2 (of 10)\n",
      "N chunks                 2\n",
      "Total time               9.29 seconds\n",
      "Total thru               1.57 MB\n",
      "Rate                     169.25 KB / sec\n",
      "Hz                       0.2151851417476836\n",
      "Progress\n",
      "Percent Complete         20.0\n",
      "Est. Time To Completion  37.18 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      4 seconds and 647.16 milliseconds\n",
      "p50                      4 seconds and 647.16 milliseconds\n",
      "p95                      8 seconds and 820.19 milliseconds\n",
      "p99                      9 seconds and 191.13 milliseconds\n",
      "-----------------------  ---------------------------------\n",
      "2019-12-27 21:02:30,067\toarph 336 : \n",
      "Partition 4 [Pid:336 Id:140641112145704]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.76 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        89.8 KB / sec\n",
      "Hz          0.11417568922247091\n",
      "----------  -------------------\n",
      "\n",
      "2019-12-27 21:02:30,140\toarph 336 : Done reading partition 9, stats:\n",
      " Partition 9 [Pid:336 Id:140641112142736]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.84 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        88.94 KB / sec\n",
      "Hz          0.11307729912366592\n",
      "----------  -------------------\n",
      "2019-12-27 21:02:30,147\toarph 336 : Progress for \n",
      "spark_tf_dataset [Pid:336 Id:140639257511920]\n",
      "-----------------------  ---------------------------------\n",
      "Thruput\n",
      "N thru                   3 (of 10)\n",
      "N chunks                 3\n",
      "Total time               9.37 seconds\n",
      "Total thru               2.36 MB\n",
      "Rate                     251.83 KB / sec\n",
      "Hz                       0.3201814097500565\n",
      "Progress\n",
      "Percent Complete         30.0\n",
      "Est. Time To Completion  21.86 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      3 seconds and 123.23 milliseconds\n",
      "p50                      75.37 milliseconds\n",
      "p95                      8 seconds and 363.01 milliseconds\n",
      "p99                      9 seconds and 99.69 milliseconds\n",
      "-----------------------  ---------------------------------\n",
      "2019-12-27 21:02:30,150\toarph 336 : \n",
      "Partition 9 [Pid:336 Id:140641112142736]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.84 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        88.94 KB / sec\n",
      "Hz          0.11307729912366592\n",
      "----------  -------------------\n",
      "\n",
      "2019-12-27 21:02:30,167\toarph 336 : Done reading partition 3, stats:\n",
      " Partition 3 [Pid:336 Id:140641112114512]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.88 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        88.53 KB / sec\n",
      "Hz          0.11255886336132385\n",
      "----------  -------------------\n",
      "2019-12-27 21:02:30,175\toarph 336 : Progress for \n",
      "spark_tf_dataset [Pid:336 Id:140639257511920]\n",
      "-----------------------  ---------------------------------\n",
      "Thruput\n",
      "N thru                   4 (of 10)\n",
      "N chunks                 4\n",
      "Total time               9.39 seconds\n",
      "Total thru               3.15 MB\n",
      "Rate                     335.05 KB / sec\n",
      "Hz                       0.42598915263832476\n",
      "Progress\n",
      "Percent Complete         40.0\n",
      "Est. Time To Completion  14.08 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      2 seconds and 347.48 milliseconds\n",
      "p50                      47.79 milliseconds\n",
      "p95                      7 seconds and 902.59 milliseconds\n",
      "p99                      9 seconds and 7.61 milliseconds\n",
      "-----------------------  ---------------------------------\n",
      "2019-12-27 21:02:30,180\toarph 336 : \n",
      "Partition 3 [Pid:336 Id:140641112114512]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.88 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        88.53 KB / sec\n",
      "Hz          0.11255886336132385\n",
      "----------  -------------------\n",
      "\n",
      "2019-12-27 21:02:30,191\toarph 336 : Done reading partition 1, stats:\n",
      " Partition 1 [Pid:336 Id:140641112114288]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.9 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        88.35 KB / sec\n",
      "Hz          0.11232812819197387\n",
      "----------  -------------------\n",
      "2019-12-27 21:02:30,196\toarph 336 : Progress for \n",
      "spark_tf_dataset [Pid:336 Id:140639257511920]\n",
      "-----------------------  ---------------------------------\n",
      "Thruput\n",
      "N thru                   5 (of 10)\n",
      "N chunks                 5\n",
      "Total time               9.4 seconds\n",
      "Total thru               3.93 MB\n",
      "Rate                     418.21 KB / sec\n",
      "Hz                       0.5317270879443539\n",
      "Progress\n",
      "Percent Complete         50.0\n",
      "Est. Time To Completion  9.4 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      1 second and 880.66 milliseconds\n",
      "p50                      20.22 milliseconds\n",
      "p95                      7 seconds and 442.16 milliseconds\n",
      "p99                      8 seconds and 915.52 milliseconds\n",
      "-----------------------  ---------------------------------\n",
      "2019-12-27 21:02:30,197\toarph 336 : \n",
      "Partition 1 [Pid:336 Id:140641112114288]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.9 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        88.35 KB / sec\n",
      "Hz          0.11232812819197387\n",
      "----------  -------------------\n",
      "\n",
      "2019-12-27 21:02:30,227\toarph 336 : Done reading partition 6, stats:\n",
      " Partition 6 [Pid:336 Id:140641112116360]\n",
      "----------  ------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.94 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        88.01 KB / sec\n",
      "Hz          0.1118984490328969\n",
      "----------  ------------------\n",
      "2019-12-27 21:02:30,232\toarph 336 : Progress for \n",
      "spark_tf_dataset [Pid:336 Id:140639257511920]\n",
      "-----------------------  ---------------------------------\n",
      "Thruput\n",
      "N thru                   6 (of 10)\n",
      "N chunks                 6\n",
      "Total time               9.43 seconds\n",
      "Total thru               4.72 MB\n",
      "Rate                     500.18 KB / sec\n",
      "Hz                       0.6359411237617654\n",
      "Progress\n",
      "Percent Complete         60.0\n",
      "Est. Time To Completion  6.29 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      1 second and 572.47 milliseconds\n",
      "p50                      25.87 milliseconds\n",
      "p95                      6 seconds and 981.74 milliseconds\n",
      "p99                      8 seconds and 823.44 milliseconds\n",
      "-----------------------  ---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-27 21:02:30,233\toarph 336 : \n",
      "Partition 6 [Pid:336 Id:140641112116360]\n",
      "----------  ------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  8.94 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        88.01 KB / sec\n",
      "Hz          0.1118984490328969\n",
      "----------  ------------------\n",
      "\n",
      "2019-12-27 21:02:30,568\toarph 336 : Done reading partition 8, stats:\n",
      " Partition 8 [Pid:336 Id:140641112117200]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  9.28 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        84.78 KB / sec\n",
      "Hz          0.10779677057703793\n",
      "----------  -------------------\n",
      "2019-12-27 21:02:30,572\toarph 336 : Done reading partition 7, stats:\n",
      " Partition 7 [Pid:336 Id:140641112142008]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  9.28 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        84.77 KB / sec\n",
      "Hz          0.10777922258165155\n",
      "----------  -------------------\n",
      "2019-12-27 21:02:30,606\toarph 336 : Done reading partition 0, stats:\n",
      " Partition 0 [Pid:336 Id:140641112145536]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  9.32 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        84.4 KB / sec\n",
      "Hz          0.10730318717766127\n",
      "----------  -------------------\n",
      "2019-12-27 21:02:30,625\toarph 336 : Progress for \n",
      "spark_tf_dataset [Pid:336 Id:140639257511920]\n",
      "-----------------------  ---------------------------------\n",
      "Thruput\n",
      "N thru                   7 (of 10)\n",
      "N chunks                 7\n",
      "Total time               9.81 seconds\n",
      "Total thru               5.51 MB\n",
      "Rate                     561.01 KB / sec\n",
      "Hz                       0.7132781716136398\n",
      "Progress\n",
      "Percent Complete         70.0\n",
      "Est. Time To Completion  4.21 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      1 second and 401.98 milliseconds\n",
      "p50                      31.52 milliseconds\n",
      "p95                      6 seconds and 612.41 milliseconds\n",
      "p99                      8 seconds and 749.57 milliseconds\n",
      "-----------------------  ---------------------------------\n",
      "2019-12-27 21:02:30,686\toarph 336 : Done reading partition 2, stats:\n",
      " Partition 2 [Pid:336 Id:140641112115520]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  9.39 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        83.74 KB / sec\n",
      "Hz          0.10646848954044484\n",
      "----------  -------------------\n",
      "2019-12-27 21:02:30,687\toarph 336 : \n",
      "Partition 8 [Pid:336 Id:140641112117200]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  9.28 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        84.78 KB / sec\n",
      "Hz          0.10779677057703793\n",
      "----------  -------------------\n",
      "\n",
      "2019-12-27 21:02:30,691\toarph 336 : Progress for \n",
      "spark_tf_dataset [Pid:336 Id:140639257511920]\n",
      "-----------------------  ---------------------------------\n",
      "Thruput\n",
      "N thru                   8 (of 10)\n",
      "N chunks                 8\n",
      "Total time               9.82 seconds\n",
      "Total thru               6.29 MB\n",
      "Rate                     641.03 KB / sec\n",
      "Hz                       0.8150258578040357\n",
      "Progress\n",
      "Percent Complete         80.0\n",
      "Est. Time To Completion  2.45 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      1 second and 226.95 milliseconds\n",
      "p50                      25.87 milliseconds\n",
      "p95                      6 seconds and 167.16 milliseconds\n",
      "p99                      8 seconds and 660.52 milliseconds\n",
      "-----------------------  ---------------------------------\n",
      "2019-12-27 21:02:30,693\toarph 336 : \n",
      "Partition 7 [Pid:336 Id:140641112142008]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  9.28 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        84.77 KB / sec\n",
      "Hz          0.10777922258165155\n",
      "----------  -------------------\n",
      "\n",
      "2019-12-27 21:02:30,697\toarph 336 : Progress for \n",
      "spark_tf_dataset [Pid:336 Id:140639257511920]\n",
      "-----------------------  ---------------------------------\n",
      "Thruput\n",
      "N thru                   9 (of 10)\n",
      "N chunks                 9\n",
      "Total time               9.82 seconds\n",
      "Total thru               7.08 MB\n",
      "Rate                     721.1 KB / sec\n",
      "Hz                       0.9168247665515971\n",
      "Progress\n",
      "Percent Complete         90.0\n",
      "Est. Time To Completion  1.09 second\n",
      "Latency (per chunk)\n",
      "Avg                      1 second and 90.72 milliseconds\n",
      "p50                      20.22 milliseconds\n",
      "p95                      5 seconds and 721.92 milliseconds\n",
      "p99                      8 seconds and 571.47 milliseconds\n",
      "-----------------------  ---------------------------------\n",
      "2019-12-27 21:02:30,699\toarph 336 : \n",
      "Partition 0 [Pid:336 Id:140641112145536]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  9.32 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        84.4 KB / sec\n",
      "Hz          0.10730318717766127\n",
      "----------  -------------------\n",
      "\n",
      "2019-12-27 21:02:30,704\toarph 336 : Progress for \n",
      "spark_tf_dataset [Pid:336 Id:140639257511920]\n",
      "-----------------------  ---------------------------------\n",
      "Thruput\n",
      "N thru                   10 (of 10)\n",
      "N chunks                 10\n",
      "Total time               9.82 seconds\n",
      "Total thru               7.87 MB\n",
      "Rate                     801.13 KB / sec\n",
      "Hz                       1.0185824147813816\n",
      "Progress\n",
      "Percent Complete         100.0\n",
      "Est. Time To Completion  0 seconds\n",
      "Latency (per chunk)\n",
      "Avg                      981.76 milliseconds\n",
      "p50                      16.82 milliseconds\n",
      "p95                      5 seconds and 276.68 milliseconds\n",
      "p99                      8 seconds and 482.43 milliseconds\n",
      "-----------------------  ---------------------------------\n",
      "2019-12-27 21:02:30,705\toarph 336 : \n",
      "Partition 2 [Pid:336 Id:140641112115520]\n",
      "----------  -------------------\n",
      "Thruput\n",
      "N thru      1\n",
      "N chunks    1\n",
      "Total time  9.39 seconds\n",
      "Total thru  786.52 KB\n",
      "Rate        83.74 KB / sec\n",
      "Hz          0.10646848954044484\n",
      "----------  -------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 10 records\n"
     ]
    }
   ],
   "source": [
    "udf = spark.read.parquet(DATASET_PATH)\n",
    "print(\"Have %s rows\" % udf.count())\n",
    "n_expect = udf.count()\n",
    "\n",
    "ds = spark_df_to_tf_dataset(\n",
    "        udf,\n",
    "        'part',\n",
    "        spark_row_to_tf_element=lambda r: (r.x, r.id, r.y),\n",
    "        tf_element_types=(tf.int64, tf.string, tf.float64))\n",
    "\n",
    "n = 0\n",
    "t = util.ThruputObserver(name='test_spark_df_to_tf_dataset_large')\n",
    "with util.tf_data_session(ds) as (sess, iter_dataset):\n",
    "  t.start_block()\n",
    "  for actual in iter_dataset():\n",
    "    n += 1\n",
    "    t.update_tallies(n=1)\n",
    "    for i in range(len(actual)):\n",
    "      t.update_tallies(num_bytes=sys.getsizeof(actual[i]))\n",
    "    t.maybe_log_progress()\n",
    "  t.stop_block()\n",
    "\n",
    "print(\"Read %s records\" % n)\n",
    "assert n == n_expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
