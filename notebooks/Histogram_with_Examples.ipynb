{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms with Examples\n",
    "_Peering inside Histogram Bins with Spark and Bokeh_\n",
    "\n",
    "<img src=\"https://github.com/pwais/oarphpy/blob/master/notebooks/hist-with-examples-diag.jpg?raw=true\" alt=\"hist-with-examples\" />\n",
    "\n",
    "A histogram is one of the most effective tools for exploring a new dataset.  In one graph, a histogram displays key information about the data's mean, variance, outliers, and periodic features.  Histograms are so important than several libraries make histogramming extremely easy:\n",
    " * In [Pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html), the user can immediately plot a histogram from a Dataframe in a Jupyter notebook with a single function call.\n",
    " * [Tensorboard](https://github.com/tensorflow/tensorboard/blob/master/docs/r1/histograms.md) features a powerful temporal histogramming tool that can be critical for monitoring the weights of neural networks during training and debugging high-dimensional optimization problems.\n",
    " * [Bokeh](https://demo.bokeh.org/selection_histogram) provides a web-based histogram plotter with interactive tools and a nice Python API.\n",
    "\n",
    "Histogram plots are often both surprising and boring: some bins have more items than expected, some bins have fewer, and a lot of bins are empty.  It's not unreasonable to immediately want to ask: can we peer inside a bin?  Which examples from my dataset are actually in there?  And, since this histogram only shows one dimension of the data, what might be some other dimensions or factors that are common among most of the things in that bin?\n",
    "\n",
    "This tutorial will show you how OarphPy's `HistogramWithExamplesPlotter` helps you do exactly that!  What do you need?\n",
    " 1. A DataFrame (Pandas or Spark) with at least one numeric or categorical column.\n",
    " 2. A Python function for visualizing a row (or some portion of a row).  For example, a function to convert a row to a pretty string or HTML visualization.\n",
    " 3. A Jupyter Notebook (like this one!) or a Python script to render the Bokeh HTML plot and display and/or save it to disk.\n",
    "\n",
    "Why `HistogramWithExamplesPlotter` ?\n",
    " * We'll use Spark to compute the actual histogram.  Spark provides multi-cpu (and even multi-machine) processing to make histogramming scale linearly.\n",
    " * We'll also use Spark to render visualizations for the bucket items.  Spark's RDD API helps accomodate arbitrary user visualization functions and runs computation in parallel (even across many machines).\n",
    " * We use Bokeh's Histogram tool because it supports the simple interactivity we need (click on a bucket to view examples) and plots work in any modern browser with no extra dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "To run this notebook locally, try using the `oarphpy/full` dockerized environment:\n",
    "\n",
    "```docker run -it --rm --net=host oarphpy/full:0.1.1 jupyter notebook --allow-root --ip=\"*\"```\n",
    "\n",
    "If you can't run the notebook locally, find an HTML-rendered copy [here](https://drive.google.com/file/d/1-uWxGQ7mrcY8aZMmBDc5AlV4kQAPLntR/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install oarphpy[spark]==0.1.1\n",
    "    !pip install pyspark==3.3.2\n",
    "    !apt-get update && apt-get install -y openjdk-11-jdk\n",
    "    os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Motivating Example: Exploring Out-of-Distribution Robustness in MNIST\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3744/1*SGPGG7oeSvVlV5sOSQ2iZw.png\" width=\"600\" />\n",
    "<center>An MNIST example input fed into a LeNet Network</center>\n",
    "\n",
    "\n",
    "The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset is well-studied in Computer Vision and consists of thousands of small pictures of hand-written digits.  (New to MNIST? Suppose you're the Post Office and you want to train a Computer Vision model that can read the zipcode digits that people write on their mail.  MNIST has a sample of such handwritten digits).  Today, it's easy to train a convolutional neural network on MNIST and achieve over 98% accuracy.  We're going to do exactly that in the next notebook cell!  \n",
    "\n",
    "But MNIST is a relatively small dataset versus all the digits people have ever written on paper.  How robust is a trained MNIST model to new data?  What if we don't have labels for that new data?  In this tutorial, we're going to use `HistogramWithExamplesPlotter` to examine the scores that an MNIST-trained model gives to \"corrupted\" data never seen at training time.\n",
    "\n",
    "First, let's train a basic MNIST model using Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install -v tqdm torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Basic MNIST ConvNet c/o Pytorch (with some small modifications noted)\n",
    "# https://github.com/pytorch/examples/blob/40289773aa4916fad0d50967917b3ae8aa534fd6/mnist/main.py#L1\n",
    "\n",
    "model_ckpt = '/opt/mnist_cnn.pt'\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if 'google.colab' in sys.modules:\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# We need this many epochs to get a nice bimodal score distribution for the 7 class\n",
    "N_EPOCHS = 15\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Use softmax instead for easier interpretation of logits\n",
    "        # output = F.log_softmax(x, dim=1)\n",
    "        output = F.softmax(x, dim=1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    iter_train = tqdm(enumerate(train_loader), desc='train_batches', total=len(train_loader))\n",
    "    for batch_idx, (data, target) in iter_train:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        # Push log into the loss instead of net output\n",
    "        #loss = F.nll_loss(output, target)\n",
    "        loss = F.nll_loss(torch.log(output), target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        iter_test = tqdm(test_loader, desc='test_batches', total=len(test_loader))\n",
    "        for data, target in iter_test:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "train_kwargs = {'batch_size': 128}\n",
    "test_kwargs = {'batch_size': 1024}\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "dataset1 = datasets.MNIST('/opt/mnist-data', train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST('/opt/mnist-data', train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "if os.path.exists(model_ckpt):\n",
    "    print(f\"Resuming from existing checkpoint {model_ckpt}\")\n",
    "    print(f\"To re-train, delete the checkpoint: $ rm {model_ckpt}\")\n",
    "    model.load_state_dict(torch.load(model_ckpt))\n",
    "else:\n",
    "    print(f\"Training and saving to {model_ckpt}\")\n",
    "\n",
    "    for epoch in tqdm(range(1, N_EPOCHS + 1), desc='epoch'):\n",
    "        train(model, train_loader, optimizer, epoch)\n",
    "        test(model, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), model_ckpt)\n",
    "\n",
    "x_test = torch.cat([xx[0] for xx in test_loader])\n",
    "\n",
    "def model_predict(x):\n",
    "    model.cpu().eval()\n",
    "    with torch.no_grad():\n",
    "        prob = model(x)\n",
    "        pred = prob.argmax(dim=1, keepdim=True)\n",
    "    return prob, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the trained model on the test set and pack the predictions into a Pandas DataFrame.  For each input, the network outputs a score for each class (the numbers 0 through 9).  We'll take a look at the raw network scores for the class \"7,\" which is easy to confused for a \"1\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob, pred = model_predict(x_test)\n",
    "\n",
    "rows = []\n",
    "for i, (x_i, score_i, pred_i) in enumerate(zip(x_test, prob, pred)):\n",
    "    row = {}\n",
    "    for classname, score in enumerate(score_i):\n",
    "        row[f'score_{classname}'] = score.item()\n",
    "    row['x_i'] = x_i.squeeze().tolist()\n",
    "    rows.append(row)\n",
    "\n",
    "import pandas as pd\n",
    "prediction_df = pd.DataFrame(rows)\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's histogram the network's scores for the 7 class.  Note that since the network has high accuracy, the scores are rather cleanly bi-modal.  For this stage, we'll use the `pandas` built-in `hist()` feature, which gives us a histogram (though without examples or other visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "prediction_df['score_7'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Out-of-Distribution Samples using MNIST-C: Corrupted MNIST\n",
    "\n",
    "MNIST-C is a benchmark dataset derived from MNIST that has synthetic corruptions.  For example, in MNIST-C, digits are rotated, blurred, speckled, etc.  We will take the normal MNIST model we trained above, run inference on MNIST-C examples, and examine how well the corruption-unaware network generalizes.  This scenario simulates a common situation in production where one has a trained model, lots of unlabeled data, and little tooling for measuring or inspecting error in the wild.  We'll see how `HistogramWithExamplesPlotter` can be a useful tool for quick exploration. \n",
    "\n",
    "First, let's get the code for using MNIST-C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /opt && ((git clone https://github.com/pwais/oarphpy-mirror-mnist-c && \\\n",
    "              cd oarphpy-mirror-mnist-c && git checkout bba57e4ccc282f106907c5239958e72298451ea7) || echo \"have mnist-c\")\n",
    "import sys\n",
    "sys.path.append('/opt/oarphpy-mirror-mnist-c')\n",
    "\n",
    "# These are hard requirements of the package we need from above\n",
    "!pip3 install scikit-image==0.19.3\n",
    "!pip3 install wand scipy\n",
    "!ln -s /opt/oarphpy-mirror-mnist-c/pessimal_noise_matrix ./pessimal_noise_matrix || echo \"symlink placed\"\n",
    "!apt-get install -y libmagickwand-dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a single corrupted example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corruptions\n",
    "\n",
    "# See more corruptions here: \n",
    "# https://github.com/google-research/mnist-c/blob/bba57e4ccc282f106907c5239958e72298451ea7/corruptions.py#L57 \n",
    "\n",
    "x_to_corrupt = x_test[100].squeeze() * 255\n",
    "# x_corrupted = corruptions.speckle_noise(x_to_corrupt)\n",
    "# x_corrupted = corruptions.glass_blur(x_to_corrupt, severity=4)\n",
    "x_corrupted = corruptions.rotate(x_to_corrupt, severity=4)\n",
    "\n",
    "x_corrupted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/opt/oarphpy')\n",
    "from oarphpy.plotting import img_to_img_tag\n",
    "\n",
    "img_html = img_to_img_tag(x_to_corrupt)\n",
    "img_html_c = img_to_img_tag(x_corrupted)\n",
    "\n",
    "def show_html(html):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(html))\n",
    "show_html('<b>Original:</b>' + img_html)\n",
    "show_html('<b>Corrupted:</b>' + img_html_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generated corrupted versions for the entire MNIST test set, and run inference of our earlier model on these corrupted examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_test_c = np.zeros_like(x_test)\n",
    "for i in tqdm(range(len(x_test_c)), total=len(x_test_c)):\n",
    "    # xform = corruptions.glass_blur\n",
    "    xform = corruptions.rotate\n",
    "    x_test_c[i][0] = (1. / 255) * xform(x_test[i].squeeze() * 255, severity=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that worked and also declare a utility function for visualizing digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TO_SHOW = 10\n",
    "\n",
    "def unit_digit_to_img_tag(x):\n",
    "    img_char = (255 * x).astype('uint8')\n",
    "    return img_to_img_tag(img_char)\n",
    "\n",
    "for r in x_test_c[:NUM_TO_SHOW, ...]:\n",
    "    x_i = r.squeeze()\n",
    "    show_html(unit_digit_to_img_tag(x_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run inference on the corrupted data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_c, pred_c = model_predict(torch.from_numpy(x_test_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate inference results into a dataframe\n",
    "rows = []\n",
    "for i, (x_i, score_i, pred_i) in enumerate(zip(x_test_c, prob_c, pred_c)):\n",
    "    row = {}\n",
    "    for classname, score in enumerate(score_i):\n",
    "        row[f'score_{classname}'] = score.item()\n",
    "    row['x_i'] = x_i.squeeze().tolist()\n",
    "    rows.append(row)\n",
    "\n",
    "import pandas as pd\n",
    "prediction_c_df = pd.DataFrame(rows)\n",
    "prediction_c_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so how did the network score the `7` class in the corrupted data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_c_df['score_7'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, that score distribution is still bi-modal, but is *much more uniform* than the plot we saw earlier.  Clearly the model is making mistakes due to the corruptions.  But what sorts of mistakes?  If we needed to select some of these examples to label, which would we choose?  Let's use `HistogramWithExamplesPlotter` to \"peer inside\" the histogram buckets of the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oarphpy.spark import NBSpark\n",
    "spark = NBSpark.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.io import show as bokeh_show\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_c_sdf = spark.createDataFrame(prediction_c_df)\n",
    "\n",
    "# To see the text representation of a Spark Dataframe try:\n",
    "# prediction_c_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from oarphpy import plotting as pl\n",
    "class MyPlotter(pl.HistogramWithExamplesPlotter):\n",
    "    NUM_BINS = 20\n",
    "    def display_bucket(self, sub_pivot, bucket_id, irows):\n",
    "        MAX_TO_VIZ = 50\n",
    "        \n",
    "        from oarphpy.plotting import img_to_img_tag\n",
    "        htmls = []\n",
    "        for row in irows:\n",
    "            htmls.append(unit_digit_to_img_tag(np.array(row['x_i'])))\n",
    "            \n",
    "            if len(htmls) > MAX_TO_VIZ:\n",
    "                break\n",
    "        \n",
    "        # Make a nice table\n",
    "        N_COLS = 25\n",
    "        from oarphpy.util import ichunked\n",
    "        trs = [\n",
    "            \"<tr>%s</tr>\" % ''.join(\"<td>%s</td>\" % ihtml for ihtml in row)\n",
    "            for row in ichunked(htmls, n=N_COLS) \n",
    "        ]\n",
    "        table_html = \"<table>%s</table>\" % ''.join(trs)\n",
    "        \n",
    "        return bucket_id, table_html\n",
    "\n",
    "plotter = MyPlotter()\n",
    "fig = plotter.run(prediction_c_sdf, 'score_7')\n",
    "bokeh_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotter.run(spark.createDataFrame(prediction_c_df), 'score_1')\n",
    "bokeh_show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two figures above examine the model's inferences on the _corrupted_ dataset.  Let's use `HistogramWithExamplesPlotter` to visualize the inference results on the original MNIST dataset to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plotter.run(spark.createDataFrame(prediction_df), 'score_7')\n",
    "bokeh_show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples of `HistogramWithExamplesPlotter`, see these rendered HTML pages: https://drive.google.com/drive/folders/1dOmkPvdFiGBMaYEddx1KK5vmCeYl2CyV?usp=sharing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
