{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automagically Shipping Custom Libraries in Spark Using OarphPy\n",
    "\n",
    "Iterative development with *simple* user code in Spark is easy: just declare some functions, create an [RDD](https://spark.apache.org/docs/2.4.4/rdd-programming-guide.html#resilient-distributed-datasets-rdds) or [DataFrame](https://spark.apache.org/docs/2.4.4/sql-programming-guide.html#datasets-and-dataframes) and go!  But if you need to iterate on a whole *library* of user code, some careful setup and configuraton is necessary.\n",
    "\n",
    "`oarphpy` offers helpful automation: `oarphpy`-managed Spark sessions automagically include a Python Egg containing the entire library wrapping the session instantiator.  You can also just point `oarphpy` at a specific directory and go.  For notebooks, `oarphpy` supports live library code updates, without having to restart the Spark session or notebook kernel.\n",
    "\n",
    "This notebook demonstrates using `oarphpy`-managed Spark with a toy library of user code: a web scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To run this notebook locally, try using the `oarphpy/full` dockerized environment:\n",
    "`docker run -it --rm --net=host oarphpy/full:0.0.2 jupyter notebook --allow-root`\n",
    "\n",
    "**COLAB ONLY** If you're running this notebook in [Google Colab](https://colab.research.google.com/), you'll need to install `oarphpy`, Spark, and Java.  Running the cell below will take care of that for you.  You might need to restart the runtime (Use the menu option: *Runtime* > *Restart runtime ...*) in order for Colab to recognize the new modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install oarphpy[spark]==0.0.2\n",
    "    !pip install pyspark==2.4.4\n",
    "    !apt-get update && apt-get install -y openjdk-8-jdk\n",
    "    os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Custom Library: A Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom library will scrape images and 'tags' from simple HTML tables like this one:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"https://pwais.github.io/oarphpy-demo-assets/image_with_tags/beach.jpg\"> beach</td>\n",
    "<td><img src=\"https://pwais.github.io/oarphpy-demo-assets/image_with_tags/bridge.jpg\"> bridge</td>\n",
    "<td><img src=\"https://pwais.github.io/oarphpy-demo-assets/image_with_tags/car.jpg\"> car</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "We'll store scraped images in a table where each row stores a single image and its annotations.  We'll use Spark here as a tool to **E**xtract data from webpages, **T**ransform it into our own data structure(s) / schemas, and **L**oad the transformed rows into some data store (e.g. the local filesystem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Code for the Library\n",
    "\n",
    "The code for our custom library could live anywhere, but for this demo we're going to put the code in an arbitrary temporary directory.  The cell below sets that up.  \n",
    "\n",
    "*Aside:* When running a Jupyter notebook, the current working directory of the Jupyter process is implicitly included in the `PYTHONPATH`.  (This is a feature, not a bug!)  We're going to put our custom library in a random temporary directory to isolate it from Jupyter and simulates having one's own code in a separate directory (e.g. perhaps the repository isolates library code from a `notebooks` directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Putting demo assets in /tmp/tmpthfz06uk_oarphpy_demo\n"
     ]
    }
   ],
   "source": [
    "# Create a random temporary directory for our library\n",
    "import os\n",
    "import tempfile\n",
    "old_cwd = os.getcwd()\n",
    "tempdir = tempfile.TemporaryDirectory(suffix='_oarphpy_demo')\n",
    "CUSTOM_LIB_SRC_DIR = tempdir.name\n",
    "print(\"Putting demo assets in %s\" % CUSTOM_LIB_SRC_DIR)\n",
    "os.chdir(CUSTOM_LIB_SRC_DIR)\n",
    "!mkdir -p mymodule\n",
    "!touch mymodule/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write the library.  Here's our webpage-scraping code, which simply uses [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) to parse image urls and tags from the pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.6/dist-packages (from beautifulsoup4->bs4) (1.9.5)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mymodule/scrape.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mymodule/scrape.py\n",
    "def get_image_tag_pairs(url):\n",
    "    import urllib.request\n",
    "    from urllib.parse import urljoin\n",
    "    from urllib.parse import urlparse\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    raw_page = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(raw_page, features=\"html.parser\")\n",
    "    for td in soup.find_all('td'):\n",
    "        tag = td.text.strip()\n",
    "        img_src = td.find('img')['src']\n",
    "        if not urlparse(img_src).scheme:\n",
    "            img_src = urljoin(url, img_src)\n",
    "        yield tag, img_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll abstract scraped image-tag pairs into a class `ImageWithAnno`, which will also represent a single row in our final table.  In this simple demo, this class mainly helps us encapsulate the code for constructing a table row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mymodule/imagewithanno.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mymodule/imagewithanno.py\n",
    "class ImageWithAnno(object):\n",
    "    def __init__(self, url='', tag='', image_bytes=None, width=None, height=None):\n",
    "        self.url = url\n",
    "        self.tag = tag\n",
    "        self.image_bytes = image_bytes\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_from_url(url):\n",
    "        import urllib.request\n",
    "        image_bytes = bytearray(urllib.request.urlopen(url).read())\n",
    "\n",
    "        # Read the image dimensions without actually decoding the jpeg\n",
    "        from oarphpy.util import get_jpeg_size\n",
    "        width, height = get_jpeg_size(image_bytes)\n",
    "        return ImageWithAnno(\n",
    "            url=url,\n",
    "            image_bytes=image_bytes,\n",
    "            width=width,\n",
    "            height=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing working directory back to /opt/oarphpy/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(\"Changing working directory back to %s\" % old_cwd)\n",
    "os.chdir(old_cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Library\n",
    "\n",
    "Let's do a brief test of the library in this notebook.  As is the custom, we modify `sys.path` to make our library importable to the notebook's `Python` process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(CUSTOM_LIB_SRC_DIR)\n",
    "\n",
    "from mymodule.scrape import get_image_tag_pairs\n",
    "test_pairs = list(get_image_tag_pairs('https://pwais.github.io/oarphpy-demo-assets/image_with_tags/page1.html'))\n",
    "test_labels = [l for l, img in test_pairs]\n",
    "assert set(['beach', 'bridge', 'car']) == set(test_labels), \\\n",
    "    \"Got unexpected labels %s\" % (test_labels,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping with Spark and OarphPy\n",
    "\n",
    "Now we're ready to scrape!  For this demo, we'll scrape these pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE_URLS = (\n",
    "    'https://pwais.github.io/oarphpy-demo-assets/image_with_tags/page1.html',\n",
    "    'https://pwais.github.io/oarphpy-demo-assets/image_with_tags/page2.html',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic Egg-ification\n",
    "\n",
    "Now we'll start a Spark session using `oarphpy`, which will automagically Egg-ify our custom library and ship that Python Egg with our job.  By default `oarphpy` tries to Egg-ify the library surrounding the calling code; this feature is ideal when running Spark jobs from scripts where the scripts and the library code live in the same Python module.  For this demo, we just need to point `oarphpy` at our temp directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-02 09:29:37,976\toarph 6004 : Using source root /tmp/tmpthfz06uk_oarphpy_demo/mymodule \n",
      "2020-02-02 09:29:38,005\toarph 6004 : Generating egg to /tmp/tmp5wzokki1_oarphpy_eggbuild ...\n",
      "2020-02-02 09:29:38,073\toarph 6004 : ... done.  Egg at /tmp/tmp5wzokki1_oarphpy_eggbuild/mymodule-0.0.0-py3.6.egg\n"
     ]
    }
   ],
   "source": [
    "from oarphpy.spark import NBSpark\n",
    "\n",
    "NBSpark.SRC_ROOT = os.path.join(CUSTOM_LIB_SRC_DIR, 'mymodule')\n",
    "spark = NBSpark.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell above should log messages confirming that `oarphpy` Egg-ified our code and gave it to Spark.  Let's now prove that step worked.  We'll show that when we import code from our library on the Spark worker (which in this case is a local Python instance running distinct from the notebook), the import works and the imported code is coming from the `oarphpy`-generated Egg.  (This feature even has an explicit [unit test](https://github.com/pwais/oarphpy/blob/28ed5764e3cdd67ae18aa2ecec241c789398ce50/oarphpy_test/fixtures/test_spark_with_custom_library.py#L57) in `oarphpy` !).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mymodule from ['/tmp/spark-16b3127a-6481-423e-8a3d-79180ec30192/userFiles-dc5890d3-9aac-4848-951d-a8d275e05e05/mymodule-0.0.0-py3.6.egg/mymodule/__init__.py']\n"
     ]
    }
   ],
   "source": [
    "def test_mymodule_is_included():\n",
    "    import mymodule\n",
    "    return mymodule.__file__\n",
    "\n",
    "from oarphpy import spark as S\n",
    "mod_paths = S.for_each_executor(spark, test_mymodule_is_included)\n",
    "print(\"Loaded mymodule from %s\" % (mod_paths,))\n",
    "assert all('.egg' in p for p in mod_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Scraping Job\n",
    "\n",
    "We'll now run a scraping job using Spark's RDD API, which is the easiest way to leverage our custom library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD of pages to scrape: ['https://pwais.github.io/oarphpy-demo-assets/image_with_tags/page1.html', 'https://pwais.github.io/oarphpy-demo-assets/image_with_tags/page2.html']\n",
      "Scraped 5 images\n"
     ]
    }
   ],
   "source": [
    "url_rdd = spark.sparkContext.parallelize(PAGE_URLS)\n",
    "print(\"RDD of pages to scrape: %s\" % (url_rdd.collect()))\n",
    "\n",
    "from mymodule.scrape import get_image_tag_pairs\n",
    "tag_img_url_rdd = url_rdd.flatMap(get_image_tag_pairs)\n",
    "  # NB: `get_image_tag_pairs` gets sent to Spark workers via `cloudpickle`\n",
    "\n",
    "def to_image_anno(tag, img_url):\n",
    "    from mymodule.imagewithanno import ImageWithAnno\n",
    "    imganno = ImageWithAnno.create_from_url(img_url)\n",
    "    imganno.tag = tag\n",
    "    return imganno\n",
    "image_anno_rdd = tag_img_url_rdd.map(lambda pair: to_image_anno(*pair))\n",
    "num_images = image_anno_rdd.count()\n",
    "\n",
    "print(\"Scraped %s images\" % num_images)\n",
    "assert num_images == 5, \"Unexpectedly saw %s images\" % num_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View / Save the results\n",
    "\n",
    "Now let's create and save that table!  Spark can automatically convert some simple Python objects to DataFrame table rows.  (`oarphpy` offers tools for slotted classes, `numpy` arrays, and more in [`RowAdapter`](https://github.com/pwais/oarphpy/blob/d62bdca8b5743be97f74b8b45fec71f72c90aa47/oarphpy/spark.py#L682)).  We'll leverage Spark's built-in type munging and Parquet support below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>image_bytes</th>\n",
       "      <th>tag</th>\n",
       "      <th>url</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>beach</td>\n",
       "      <td>https://pwais.github.io/oarphpy-demo-assets/im...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>bridge</td>\n",
       "      <td>https://pwais.github.io/oarphpy-demo-assets/im...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>car</td>\n",
       "      <td>https://pwais.github.io/oarphpy-demo-assets/im...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>challah</td>\n",
       "      <td>https://pwais.github.io/oarphpy-demo-assets/im...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>https://pwais.github.io/oarphpy-demo-assets/im...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   height                                        image_bytes      tag  \\\n",
       "0      53  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...    beach   \n",
       "1     133  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   bridge   \n",
       "2      75  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...      car   \n",
       "3      84  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...  challah   \n",
       "4      90  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...    pizza   \n",
       "\n",
       "                                                 url  width  \n",
       "0  https://pwais.github.io/oarphpy-demo-assets/im...    120  \n",
       "1  https://pwais.github.io/oarphpy-demo-assets/im...    100  \n",
       "2  https://pwais.github.io/oarphpy-demo-assets/im...    100  \n",
       "3  https://pwais.github.io/oarphpy-demo-assets/im...     80  \n",
       "4  https://pwais.github.io/oarphpy-demo-assets/im...    120  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(image_anno_rdd)\n",
    "\n",
    "# Save the results\n",
    "df.write.parquet('/tmp/demo_results', mode='overwrite')\n",
    "\n",
    "# Show the results using df.show() or as a Pandas Dataframe, which has pretty printing support in Jupyter.\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support For Live Library Code Updates\n",
    "\n",
    "Spark supports live updates for code in notebook cells.  Every time a notebook cell executes, Spark serializes the latest code (via [cloudpickle](https://github.com/cloudpipe/cloudpickle)) and sends it to the workers for execution.  No need to restart the Spark session nor the notebook kernel.  Thus you can keep entire RDDs and/or DataFrames in *cluster* memory and iteratively update code and recompute things as needed.\n",
    "\n",
    "Similary, `oarhpy.NBSpark` supports live updates for library code!  By default, `NBSpark` hooks into the notebook environment and automatically creates and ships a new Egg whenenever there are updated files on disk.  (There are some potential small performance hits, see `NBSpark.MAYBE_REBUILD_EGG_EVERY_CELL_RUN` for details and a toggle).  Note that in a non-notebook, script-oriented workflow, `oarphpy.SparkSession`s will Egg-ify user code every time a script / Spark Session launches, so live updates are not typically useful.\n",
    "\n",
    "To demonstrate this feature of `oarhpy.NBSpark`, we'll update our library code in-place and add a new image attribute / column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(CUSTOM_LIB_SRC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mymodule/imagewithanno.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mymodule/imagewithanno.py\n",
    "class ImageWithAnno(object):\n",
    "    def __init__(self, url='', tag='', image_bytes=None, width=None, height=None):\n",
    "        self.url = url\n",
    "        self.tag = tag\n",
    "        self.image_bytes = image_bytes\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "        from urllib.parse import urlparse\n",
    "        self.domain = urlparse(url).netloc  # <------------------  A new attribute / column !!\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_from_url(url):\n",
    "        import urllib.request\n",
    "        image_bytes = bytearray(urllib.request.urlopen(url).read())\n",
    "        \n",
    "        # Read the image dimensions without actually decoding the jpeg\n",
    "        from oarphpy.util import get_jpeg_size\n",
    "        width, height = get_jpeg_size(image_bytes)\n",
    "        return ImageWithAnno(\n",
    "            url=url,\n",
    "            image_bytes=image_bytes,\n",
    "            width=width,\n",
    "            height=height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-02 09:29:45,348\toarph 6004 : Source has changed! Rebuilding Egg ...\n",
      "2020-02-02 09:29:45,349\toarph 6004 : Using source root /tmp/tmpthfz06uk_oarphpy_demo/mymodule \n",
      "2020-02-02 09:29:45,350\toarph 6004 : Generating egg to /tmp/tmpffetxk5d_oarphpy_eggbuild ...\n",
      "2020-02-02 09:29:45,360\toarph 6004 : ... done.  Egg at /tmp/tmpffetxk5d_oarphpy_eggbuild/mymodule-0.0.0-py3.6.egg\n"
     ]
    }
   ],
   "source": [
    "os.chdir(old_cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to re-run the code that creates our `DataFrame`.  Since Spark RDDs and DataFrames are constructed lazily (unless [`cache()ed`](https://spark.apache.org/docs/2.4.4/api/python/pyspark.html#pyspark.RDD.cache) or [`persist()ed`](https://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html#pyspark.sql.DataFrame.persist)), the code below makes Spark re-run the code that constructs `image_anno_rdd`, which in turn will execute code in the updated Egg with our local changes.\n",
    "\n",
    "(Note that if you do `cache()` or `persist()` your RDD or DataFrame, Spark will **not** execute code in the new Egg until you do an [`unpersist()`](https://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html#pyspark.sql.DataFrame.unpersist)!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>height</th>\n",
       "      <th>image_bytes</th>\n",
       "      <th>tag</th>\n",
       "      <th>url</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pwais.github.io</td>\n",
       "      <td>53</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>beach</td>\n",
       "      <td>https://pwais.github.io/oarphpy-demo-assets/im...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pwais.github.io</td>\n",
       "      <td>133</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>bridge</td>\n",
       "      <td>https://pwais.github.io/oarphpy-demo-assets/im...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pwais.github.io</td>\n",
       "      <td>75</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>car</td>\n",
       "      <td>https://pwais.github.io/oarphpy-demo-assets/im...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pwais.github.io</td>\n",
       "      <td>84</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>challah</td>\n",
       "      <td>https://pwais.github.io/oarphpy-demo-assets/im...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pwais.github.io</td>\n",
       "      <td>90</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>https://pwais.github.io/oarphpy-demo-assets/im...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            domain  height                                        image_bytes  \\\n",
       "0  pwais.github.io      53  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   \n",
       "1  pwais.github.io     133  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   \n",
       "2  pwais.github.io      75  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   \n",
       "3  pwais.github.io      84  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   \n",
       "4  pwais.github.io      90  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   \n",
       "\n",
       "       tag                                                url  width  \n",
       "0    beach  https://pwais.github.io/oarphpy-demo-assets/im...    120  \n",
       "1   bridge  https://pwais.github.io/oarphpy-demo-assets/im...    100  \n",
       "2      car  https://pwais.github.io/oarphpy-demo-assets/im...    100  \n",
       "3  challah  https://pwais.github.io/oarphpy-demo-assets/im...     80  \n",
       "4    pizza  https://pwais.github.io/oarphpy-demo-assets/im...    120  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(image_anno_rdd)\n",
    "\n",
    "# Check that our new column is there!\n",
    "assert 'domain' in df.columns\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
